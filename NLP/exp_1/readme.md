## **ğŸ¤— NLP ì‚¬ì „í•™ìŠµ(1)**

### **ğŸ¯ Overview**
1. **ì£¼ì œ:**
- IMDB ê°ì„± ë¶„ë¥˜ë¥¼ ìœ„í•œ `Encoder` ëª¨ë¸ ë¹„êµ ì‹¤í—˜
    - [IMDB ê°ì„± ë¶„ì„(ê¸ì •/ë¶€ì • ë¶„ë¥˜)](https://huggingface.co/datasets/stanfordnlp/imdb) ì„±ëŠ¥ ë¹„êµ
    - Transformer ê¸°ë°˜ Encoder ëª¨ë¸ì˜ ë¬¸ì¥ í‘œí˜„ ë°©ì‹ì´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„
    
    > ì‹¤í—˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ëª¨ë¸ì˜ íŠ¹ì§• ë° ì ìš© ê°€ëŠ¥ì„± í‰ê°€
  
2. **í•™ìŠµ ëª¨ë¸:** 
    - BERT-base-uncased
    - ModernBERT-base
3. **ì‹¤í—˜ setting:**
    - ë°ì´í„° ë¶„í• : Train:Valid:Test = 8:1:1
    - ìµœëŒ€ í•™ìŠµ epoch: `5`
    - optimizer: `Adam`
    - lr: `5e-5`
    - max_len: `128`
    - scheduler: `constant`
    - ì‹¤í—˜ ì¬í˜„ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
    
      ```python
      from transformers import set_seed
      set_seed(42)
      ```
    
4. **ê²°ê³¼:**
    - í•™ìŠµ ì¤‘ ë§¤ epoch validation ì§„í–‰, checkpoint ì €ì¥
    - ìµœì ì˜ checkpointì— ëŒ€í•œ test ì§„í–‰
    - wandb logging í•„ìˆ˜

      
### **ğŸ“‚ íŒŒì¼ êµ¬ì¡°**
```
project/
â”‚â”€â”€ configs/                 # ì„¤ì • ê´€ë ¨ í´ë”
â”‚   â”œâ”€â”€ config.yaml          # ì„¤ì • íŒŒì¼
â”‚â”€â”€ src/                     # í•µì‹¬ ì½”ë“œ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ utils.py             # ì„¤ì • ë¡œë“œ & ìœ í‹¸ í•¨ìˆ˜
â”‚   â”œâ”€â”€ data.py              # Dataset & DataLoader ì •ì˜
â”‚   â”œâ”€â”€ model.py             # BERT ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸
â”‚   â”œâ”€â”€ main.py              # í•™ìŠµ ë° ê²€ì¦ ì‹¤í–‰
â”‚â”€â”€ checkpoints/             # ì €ì¥ëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
```

### **ğŸ› ï¸ Descriptions**

**ğŸ“Œ ì„¤ì • ë° ìœ í‹¸(**`configs/config.yaml`, `src/utils.py`**)**

- `OmegaConf`ë¥¼ í™œìš©í•œ ì„¤ì • ë¡œë“œ ë° í™˜ê²½ ë³€ìˆ˜ ì²˜ë¦¬
    - ëª¨ë¸, ë°ì´í„°, í•™ìŠµ ê´€ë ¨ ê³µí†µ ì„¤ì • ì €ì¥
        - `data`, `train`, `torch`, `logging` ë“±ì˜ ê³µí†µ í™˜ê²½ ì„¤ì • ìœ ì§€
- `get_model_name()`ì„ í†µí•´ BERTì™€ ModernBERTì˜ ëª¨ë¸ëª… ìë™ ë§¤í•‘
    
    ```python
    model_mapping = {
            "bert": "bert-base-uncased",
            "modernbert": "answerdotai/ModernBERT-base"
        }
    ```
    
- ë¡œê¹… ë° ì‹¤í—˜ ê¸°ë¡: `set_logger()`, `wandb_logger()`

---

**ğŸ“Œ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬(**`src/data.py`**)**

- IMDB ë°ì´í„° ë¡œë“œ ë° ë³‘í•© â†’ ë¶„ë¦¬
    - Train:Valid:Test = 8:1:1
- `AutoTokenizer`ë¥¼ í™œìš©í•œ í† í°í™”
    - `padding='max_length', truncation=True, max_length=128`
- `PyTorch Dataset` ë° `DataLoader` ì •ì˜
    - BERTì™€ ModernBERTì˜ `token_type_ids` ì°¨ì´ ì²˜ë¦¬
        
        ```python
        # BERT ëª¨ë¸ì¼ ë•Œë§Œ token_type_ids í¬í•¨
        if self.data_config.model.model_name.lower() == "bert-base-uncased":
            input_data["token_type_ids"] = torch.tensor(self.data["token_type_ids"][idx], dtype=torch.long)
        ```
        
    - `collate_fn()` í™œìš©í•˜ì—¬ batch tensor ë³€í™˜
        ```
        Testing: Data Loading ...
        Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40000/40000 [00:07<00:00, 5407.17 examples/s]
        >> SPLIT: train | Total Data Length: 40000
        Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 5513.82 examples/s]
        >> SPLIT: valid | Total Data Length: 5000
        Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 5890.03 examples/s]
        >> SPLIT: test | Total Data Length: 5000
        âœ… Train DataLoader Size: 5000 # 40000 / 8
        âœ… Valid DataLoader Size: 625 # 5000 / 8
        âœ… Test DataLoader Size: 1250 # 5000 / 4
        
        ğŸ”¹ First batch sample:
        ğŸ”¸ Input IDs shape: torch.Size([8, 128])
        ğŸ”¸ Attention Mask shape: torch.Size([8, 128])
        ğŸ”¸ Labels shape: torch.Size([8])
        ```
---

**ğŸ“Œ ëª¨ë¸ ì •ì˜**(`src/model.py`)

- `BERT` ë° `ModernBERT`ì˜ ë¶„ë¥˜ ëª¨ë¸ êµ¬í˜„
    - `token_type_ids` í™œìš© ìœ ë¬´ ë°˜ì˜
- ë¬¸ì¥ í‘œí˜„ ë°©ì‹ ì°¨ì´ì :
    - initial setting: `[CLS]`ë¥¼ ëŒ€í‘œ í† í°ìœ¼ë¡œ í•™ìŠµ
        
        ```python
        pooled_output = outputs.last_hidden_state[:, 0, :]
        ```
        
        â†’ `[CLS]`ê°€ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ë‹´ì§€ ëª»í•  ìˆ˜ë„ ìˆìŒ  
        â†’ ì‹¤ì œë¡œ BERT ì´ˆê¸° í•™ìŠµì´ ë§¤ìš° ë¶ˆì•ˆì •í•˜ì˜€ìŒ(ì •í™•ë„: 50% ì •ë„)  
        
    - 2nd_ver: ë¬¸ì¥ ì „ì²´ í‰ê·  (`torch.mean()`) ì‚¬ìš©
        
        ```python
        pooled_output = torch.mean(outputs.last_hidden_state, dim = 1)
        ```
        
        â†’ ë¬¸ì¥ ì „ì²´ì˜ ì •ë³´ë¥¼ í‰ê· ì ìœ¼ë¡œ ë°˜ì˜í•˜ê¸°ì— [CLS] token í•˜ë‚˜ë³´ë‹¤ ë” ì•ˆì •ì ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŒ  
        (ì‹¤ì œ ì„±ëŠ¥ ê°œì„ : 30%ì •ë„)  
        â†’ BERTì™€ ModernBERT ê°„ êµ¬ì¡°ì  ì°¨ì´ë¥¼ ì¤„ì´ê³ , ì„±ëŠ¥ ì°¨ì´ ìµœì†Œí™”
        

---

**ğŸ“Œ í•™ìŠµ ë° ê²€ì¦(**`src/main.py`**)**

- `config.yaml`ì„ ë¡œë“œí•˜ì—¬ ì„¤ì • ì ìš©
- `scheduler` ì •ì˜ ë° ì ìš©
- ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ í›„ í•™ìŠµ ì§„í–‰
    - `train_iter()` & `valid_iter()`
- Validation ì§„í–‰ í›„ Best Checkpoint ì €ì¥
    
    ```python
    if total_valid_accuracy > best_valid_accuracy:
        best_valid_accuracy = total_valid_accuracy
        torch.save(model.state_dict(), checkpoint_path)
    
    ```

### **ğŸ“Š ì‹¤í—˜ ê²°ê³¼**

| ëª¨ë¸ | Train Accuracy | Validation Accuracy | Test Accuracy |
| --- | --- | --- | --- |
| **BERT-base-uncased** | 0.9713 | 0.8854 | 0.8804 |
| **ModernBERT-base** | 0.9865 | 0.9114 | 0.909 |

â‡’ ModernBERTê°€ ì•½ê°„ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŒ  
â‡’ ê¸°ì¡´ `[CLS]` ë°©ì‹ì—ì„œëŠ” BERT ì„±ëŠ¥ì´ ë‚®ì•˜ìœ¼ë‚˜, `torch.mean()` ì ìš© í›„ ModernBERTì™€ ìœ ì‚¬í•´ì§

![image](https://github.com/user-attachments/assets/fea241f0-094c-4c73-bb6c-74c32d7356a4)
![image](https://github.com/user-attachments/assets/f49977b2-1ca8-4892-ad83-f0ae533742a5)

- BERT(blue), ModernBERT(purple)

### **ğŸ“ ê²°ê³¼ ë¶„ì„**

**Papers:**  
- BERT
  - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)  
- ModernBERT
  - [Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](https://arxiv.org/abs/2412.13663)
  - [Reference_blog](https://blog.sionic.ai/modernbert)

---

**1ï¸âƒ£ BERT vs ModernBERT: ì£¼ìš” ì°¨ì´ì **

|  | **BERT-base-uncased** | **ModernBERT-base** |
| --- | --- | --- |
| **Pooling ë°©ì‹** | `pooler_output` ì‚¬ìš© (`tanh(W[CLS])`) | Pooling Layer ì—†ìŒ |
| **ë¬¸ì¥ í‘œí˜„ ë°©ì‹** | `[CLS]` í† í°ì„ ëŒ€í‘œê°’ìœ¼ë¡œ ì‚¬ìš© | `last_hidden_state`ì—ì„œ ì§ì ‘ íŠ¹ì„± ì¶”ì¶œ |
| **êµ¬ì¡°ì  íŠ¹ì§•** | í‘œì¤€ Transformer Encoder | BERT êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ Pooling Layer ì œê±° |
| **ëª¨ë¸ ê²½ëŸ‰í™”** | í‘œì¤€ BERTë³´ë‹¤ ì•½ê°„ ë¬´ê±°ì›€ | Pooling Layer ì œê±°ë¡œ ì—°ì‚°ëŸ‰ ê°ì†Œ |
| **Fine-tuning ì ìš©** | íŠ¹ì • íƒœìŠ¤í¬ì— ë§ê²Œ `pooler_output` í•™ìŠµ ê°€ëŠ¥ | Pooling Layerê°€ ì—†ìœ¼ë¯€ë¡œ ì§ì ‘ íŠ¹ì„± ì¶”ì¶œ í•„ìš” |
- BERTëŠ” Pooling Layerë¥¼ í†µí•´ `[CLS]` í† í°ì„ ì¶”ê°€ì ìœ¼ë¡œ ê°€ê³µí•˜ì—¬ ë¬¸ì¥ í‘œí˜„ì„ ìƒì„±í•¨
- ModernBERTëŠ” `pooler_output` ì—†ì´ë„ ì¶©ë¶„íˆ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë„ë¡ ì„¤ê³„ë¨

---

**2ï¸âƒ£ ì„±ëŠ¥ ì°¨ì´ê°€ ë°œìƒí•œ ì›ì¸**  
- BERTì˜ `[CLS]` í† í°
    - BERTëŠ” `[CLS]` í† í°ì´ ë¬¸ì¥ ì „ì²´ì˜ ì˜ë¯¸ë¥¼ ë‹´ë„ë¡ í•™ìŠµë˜ì§€ë§Œ, ì´ˆê¸° í•™ìŠµ ë‹¨ê³„ì—ì„œ ì¶©ë¶„í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ì§€ ëª»í•  ê°€ëŠ¥ì„±ì´ ìˆìŒ
    - ì¼ë¶€ ë…¼ë¬¸ì—ì„œëŠ” â€œ[CLS] í† í°ì´ í•­ìƒ ìµœì ì˜ ë¬¸ì¥ í‘œí˜„ì´ ì•„ë‹ ìˆ˜ ìˆë‹¤"ëŠ” ì ì„ ì§€ì 
- ModernBERTì˜ ê°œì„ ì :
    - ModernBERTì—ì„œëŠ” Pooling Layer ì—†ì´ë„ ì¶©ë¶„í•œ í‘œí˜„ë ¥ì„ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤ê³  ì£¼ì¥
        - ì¦‰, ModernBERTëŠ” BERTì˜ `[CLS]` í† í°ì„ ë”°ë¡œ ê°€ê³µí•˜ì§€ ì•Šì•„ë„ ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë¨
    - ì‹¤í—˜ì—ì„œë„ BERTì—ì„œ `torch.mean()`ì„ ì ìš©í•˜ë©´ ModernBERTì™€ ê±°ì˜ ë™ì¼í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŒ  
        â†’ ì´ëŠ” `[CLS]` í† í°ì´ ë¬¸ì¥ ì˜ë¯¸ë¥¼ ì˜¨ì „íˆ ë°˜ì˜í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê°€ì„¤ì„ ë’·ë°›ì¹¨í•¨
        - `[CLS]`ë§Œ ì‚¬ìš©í•˜ë©´ íŠ¹ì • íŒ¨í„´(ì£¼ë¡œ ì²« ë²ˆì§¸ ë‹¨ì–´)ë§Œ ë°˜ì˜ë  ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        - ê²°ê³¼ì ìœ¼ë¡œ BERTì—ì„œë„ `pooler_output` ì—†ì´ë„ ì¶©ë¶„í•œ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆìŒì„ ì…ì¦

**3ï¸âƒ£ ì–´ë–¤ ëª¨ë¸ì´ ë” ì¢‹ì€ê°€?**

- `ModernBERT`ê°€ ë” ë‚˜ì€ ê²½ìš°
    - ì—°ì‚° ì†ë„ë¥¼ ì¤„ì´ë©´ì„œë„ BERTì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ì›í•  ë•Œ
    - pooling layer ì—†ì´ë„ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•˜ê³ ì í•  ë•Œ
    - í•™ìŠµ ë° fine-tuning ê³¼ì •ì—ì„œ ì¶”ê°€ì ì¸ pooling layer ì—†ì´ ì§ì ‘ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ê³  ì‹¶ì„ ë•Œ
    - ë°°í¬ í™˜ê²½ì—ì„œ íš¨ìœ¨ì ì¸ ëª¨ë¸ì„ ì›í•  ë•Œ
        - ê²½ëŸ‰í™”ëœ ëª¨ë¸ â†’ ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ì— ìœ ë¦¬
- ê¸°ì¡´ `BERT`ê°€ ë” ìœ ë¦¬í•œ ê²½ìš°
    - ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ì„ ìœ ì§€í•´ì•¼ í•  ë•Œ
    - íŠ¹ì • íƒœìŠ¤í¬ì—ì„œ `[CLS]` í† í°ì„ í™œìš©í•˜ëŠ” ê²ƒì´ ìœ ë¦¬í•  ë•Œ
    - fine-tuning ì‹œ ì¶”ê°€ì ì¸ í•™ìŠµì´ í•„ìš”í•œ ê²½ìš°
        - ë¬¸ì„œ ë¶„ë¥˜ì—ì„œ `[CLS]`ë¥¼ í™œìš©í•œ í•™ìŠµì´ ì´ë¯¸ ì§„í–‰ëœ ê²½ìš°

---

**4ï¸âƒ£ ì‹¤ì œ ì ìš© ì‹œ ê³ ë ¤í•  ì **

1. BERTë¥¼ ì‚¬ìš©í•œë‹¤ë©´ `torch.mean()` ë°©ì‹ì„ ì¶”ì²œ
    - `[CLS]` í† í°ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ë¬¸ì¥ ì „ì²´ë¥¼ í‰ê· ë‚´ëŠ” ê²ƒì´ ë” ì•ˆì •ì ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
2. ModernBERTì—ì„œëŠ” íŠ¹ì„± ì¶”ì¶œ ë°©ì‹ì„ ê³ ë ¤í•´ì•¼ í•¨
    - pooling layerê°€ ì—†ìœ¼ë¯€ë¡œ `pooler_output`ì„ ê¸°ëŒ€í•˜ë©´ ì•ˆë˜ë©°, `[CLS]` ë˜ëŠ” ë¬¸ì¥ ì „ì²´ í‰ê· ì„ í™œìš©í•´ì•¼ í•¨
    - ì¼ë¶€ íƒœìŠ¤í¬ì—ì„œ pooling layerê°€ í•„ìš”í•  ê²½ìš°, ModernBERT ìœ„ì— ì¶”ê°€ì ì¸ Layerë¥¼ ìŒ“ì•„ì•¼ í•  ìˆ˜ë„ ìˆìŒ
3. ì—°ì‚°ëŸ‰ì´ ì¤‘ìš”í•œ ê²½ìš° ModernBERTê°€ ë” ìœ ë¦¬
    - ModernBERTëŠ” BERTë³´ë‹¤ ì•½ 10~15% ì ì€ ì—°ì‚°ëŸ‰
    - ë”°ë¼ì„œ, ì‹¤ì‹œê°„ ì²˜ë¦¬ ë˜ëŠ” edge computing í™˜ê²½ì—ì„œëŠ” ModernBERTê°€ ë” ì í•©í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ
